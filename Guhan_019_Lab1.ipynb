{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNgmHJxTaKZZGd3MSy0YbxB",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Guhan2348519/demo/blob/main/Guhan_019_Lab1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "TOKENIZATION\n",
        "\n",
        "Tokenization is breaking the raw text into small chunks. Tokenization breaks the raw text into words, sentences called tokens. These tokens help in understanding the context or developing the model for the NLP. The tokenization helps in interpreting the meaning of the text by analyzing the sequence of the words.\n",
        "\n",
        "Tokenization can separate sentences, words, characters, or subwords. When we split the text into sentences, we call it sentence tokenization. For words, we call it word tokenization.\n",
        "\n",
        "Tokenizer breaks unstructured data and natural language text into chunks of information that can be considered as discrete elements. The token occurrences in a document can be used directly as a vector representing that document."
      ],
      "metadata": {
        "id": "581u4m7ww13e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import (word_tokenize,\n",
        "sent_tokenize, TreebankWordTokenizer, wordpunct_tokenize, TweetTokenizer,\n",
        "MWETokenizer)"
      ],
      "metadata": {
        "id": "9jOk_cmRw3kV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "505d370e-1637-47e4-ace2-939a7fbaa32d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The code imports NLTK and downloads the 'punkt' tokenizer data. It then imports six tokenizers from NLTK: `word_tokenize`, `sent_tokenize`, `TreeBankWordTokenizer`, `wordpunct_tokenize`, `TweetTokenisation`, and `MWETokenizer`. These tokenizers are useful for various text preprocessing tasks in natural language processing."
      ],
      "metadata": {
        "id": "cvwD2CL4H05X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text=\"In the heart of a sun-kissed village, curious squirrel Zara embarked on an adventure. Guided by Luna‚Äôs riddles and enchanted by a hidden waterfall, she climbed the mystical mountain.‚ù§Ô∏è‚Äçüî• At the summit, she marveled at the vibrant sky, whispered secrets to a banana-shaped cloudüí≠üî≥, and dreamt under a starry blanketüî≥. Zara‚Äôs tale reminds us that life weaves magic in ordinary moments.üòç \""
      ],
      "metadata": {
        "id": "6L4RrK-V0cmW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "SENTENCE TOKENIZATION\n",
        "\n",
        "also known as sentence segmentation or sentence boundary detection, is the process of breaking a continuous stream of text into individual sentences. In NLP, sentence tokenization is an important preprocessing step because many language analysis tasks, such as sentiment analysis, machine translation, and text summarization at the sentence level."
      ],
      "metadata": {
        "id": "Vf5visyEJupf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(word_tokenize (text))\n",
        "print(sent_tokenize(text))"
      ],
      "metadata": {
        "id": "LNHha9fexfra"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "WORD TOKENIZATION\n",
        "\n",
        "The text is broken down into individual words and sentences, which can be useful for various natural language processing tasks such as text analysis, sentiment analysis, and information retrieval. Additionally, the presence of emojis and special characters in the text is handled appropriately during tokenization."
      ],
      "metadata": {
        "id": "WT4GesALJSFv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text.split()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bQ7wG6lQsINM",
        "outputId": "27e26018-ea3c-4ab1-a369-58ea73c2c800"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['In',\n",
              " 'the',\n",
              " 'heart',\n",
              " 'of',\n",
              " 'a',\n",
              " 'sun-kissed',\n",
              " 'village,',\n",
              " 'curious',\n",
              " 'squirrel',\n",
              " 'Zara',\n",
              " 'embarked',\n",
              " 'on',\n",
              " 'an',\n",
              " 'adventure.',\n",
              " 'Guided',\n",
              " 'by',\n",
              " 'Luna‚Äôs',\n",
              " 'riddles',\n",
              " 'and',\n",
              " 'enchanted',\n",
              " 'by',\n",
              " 'a',\n",
              " 'hidden',\n",
              " 'waterfall,',\n",
              " 'she',\n",
              " 'climbed',\n",
              " 'the',\n",
              " 'mystical',\n",
              " 'mountain.‚ù§Ô∏è\\u200düî•',\n",
              " 'At',\n",
              " 'the',\n",
              " 'summit,',\n",
              " 'she',\n",
              " 'marveled',\n",
              " 'at',\n",
              " 'the',\n",
              " 'vibrant',\n",
              " 'sky,',\n",
              " 'whispered',\n",
              " 'secrets',\n",
              " 'to',\n",
              " 'a',\n",
              " 'banana-shaped',\n",
              " 'cloudüí≠üî≥,',\n",
              " 'and',\n",
              " 'dreamt',\n",
              " 'under',\n",
              " 'a',\n",
              " 'starry',\n",
              " 'blanketüî≥.',\n",
              " 'Zara‚Äôs',\n",
              " 'tale',\n",
              " 'reminds',\n",
              " 'us',\n",
              " 'that',\n",
              " 'life',\n",
              " 'weaves',\n",
              " 'magic',\n",
              " 'in',\n",
              " 'ordinary',\n",
              " 'moments.üòç']"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The output is a list representing the individual words and punctuation marks from the input text, separated by whitespace. It accurately captures the structure of the original text, preserving elements such as emojis and punctuation."
      ],
      "metadata": {
        "id": "dwlFHB-RNFNk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(wordpunct_tokenize (text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HEfwg5XDstCG",
        "outputId": "f2074e02-6a7f-4980-e092-01b6a1d5237b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['In', 'the', 'heart', 'of', 'a', 'sun', '-', 'kissed', 'village', ',', 'curious', 'squirrel', 'Zara', 'embarked', 'on', 'an', 'adventure', '.', 'Guided', 'by', 'Luna', '‚Äô', 's', 'riddles', 'and', 'enchanted', 'by', 'a', 'hidden', 'waterfall', ',', 'she', 'climbed', 'the', 'mystical', 'mountain', '.‚ù§Ô∏è\\u200düî•', 'At', 'the', 'summit', ',', 'she', 'marveled', 'at', 'the', 'vibrant', 'sky', ',', 'whispered', 'secrets', 'to', 'a', 'banana', '-', 'shaped', 'cloud', 'üí≠üî≥,', 'and', 'dreamt', 'under', 'a', 'starry', 'blanket', 'üî≥.', 'Zara', '‚Äô', 's', 'tale', 'reminds', 'us', 'that', 'life', 'weaves', 'magic', 'in', 'ordinary', 'moments', '.üòç']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "Punctuation-based tokenizer\n",
        "\n",
        "This tokenizer splits the sentences into words based on whitespaces and punctuations\n",
        "\n",
        "\n",
        "The output is a list of words and punctuation marks extracted from the input text using NLTK's `wordpunct_tokenize` function. Each word and punctuation mark is treated as a separate token, including special characters like emojis. The text is tokenized based on whitespace and punctuation marks, resulting in a detailed breakdown of the input text into individual elements."
      ],
      "metadata": {
        "id": "qCu3cDQVNV4s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "doc = nlp(text)\n",
        "for token in doc:\n",
        " print(token, token.idx)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "phkN3Ks5K7ti",
        "outputId": "087ef248-b174-4a4e-95fd-b9628ea7176a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "In 0\n",
            "the 3\n",
            "heart 7\n",
            "of 13\n",
            "a 16\n",
            "sun 18\n",
            "- 21\n",
            "kissed 22\n",
            "village 29\n",
            ", 36\n",
            "curious 38\n",
            "squirrel 46\n",
            "Zara 55\n",
            "embarked 60\n",
            "on 69\n",
            "an 72\n",
            "adventure 75\n",
            ". 84\n",
            "Guided 86\n",
            "by 93\n",
            "Luna 96\n",
            "‚Äôs 100\n",
            "riddles 103\n",
            "and 111\n",
            "enchanted 115\n",
            "by 125\n",
            "a 128\n",
            "hidden 130\n",
            "waterfall 137\n",
            ", 146\n",
            "she 148\n",
            "climbed 152\n",
            "the 160\n",
            "mystical 164\n",
            "mountain. 173\n",
            "‚ù§ 182\n",
            "Ô∏è‚Äç 183\n",
            "üî• 185\n",
            "At 187\n",
            "the 190\n",
            "summit 194\n",
            ", 200\n",
            "she 202\n",
            "marveled 206\n",
            "at 215\n",
            "the 218\n",
            "vibrant 222\n",
            "sky 230\n",
            ", 233\n",
            "whispered 235\n",
            "secrets 245\n",
            "to 253\n",
            "a 256\n",
            "banana 258\n",
            "- 264\n",
            "shaped 265\n",
            "cloud 272\n",
            "üí≠ 277\n",
            "üî≥ 278\n",
            ", 279\n",
            "and 281\n",
            "dreamt 285\n",
            "under 292\n",
            "a 298\n",
            "starry 300\n",
            "blanket 307\n",
            "üî≥ 314\n",
            ". 315\n",
            "Zara 317\n",
            "‚Äôs 321\n",
            "tale 324\n",
            "reminds 329\n",
            "us 337\n",
            "that 340\n",
            "life 345\n",
            "weaves 350\n",
            "magic 357\n",
            "in 363\n",
            "ordinary 366\n",
            "moments 375\n",
            ". 382\n",
            "üòç 383\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "SpaCy\n",
        "\n",
        "\n",
        " is a Python library for NLP tasks, offering efficient parsing and understanding of text. It supports multiple languages and allows customization for tokenization, such as preserving special tokens like \"U.K.\" while splitting punctuation. Before use, spaCy requires installation and downloading English language data and models.\n",
        "\n",
        "\n",
        "Here's an explanation of what each part of the code does:\n",
        "1.doc = nlp(text): Processes the input text with the spaCy pipeline, creating a Doc object containing the analyzed tokens.\n",
        "2.for token in doc:: Iterates through each token in the Doc object.\n",
        "3.print(token, token.idx): Prints the token and its starting index within the original text.\n",
        "\n",
        "The output consists of each token and its corresponding index within the text, which can be useful for various text processing tasks like entity recognition, part-of-speech tagging, and syntactic parsing. Each token is printed along with its index, allowing for a detailed analysis of the text structure."
      ],
      "metadata": {
        "id": "fVx-8zrNNqtq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer=TreebankWordTokenizer()\n",
        "print(tokenizer.tokenize (text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mIdR-E9Ds58l",
        "outputId": "e9bcd189-eb0d-48c1-d2e0-10e262345bcf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['In', 'the', 'heart', 'of', 'a', 'sun-kissed', 'village', ',', 'curious', 'squirrel', 'Zara', 'embarked', 'on', 'an', 'adventure.', 'Guided', 'by', 'Luna‚Äôs', 'riddles', 'and', 'enchanted', 'by', 'a', 'hidden', 'waterfall', ',', 'she', 'climbed', 'the', 'mystical', 'mountain.‚ù§Ô∏è\\u200düî•', 'At', 'the', 'summit', ',', 'she', 'marveled', 'at', 'the', 'vibrant', 'sky', ',', 'whispered', 'secrets', 'to', 'a', 'banana-shaped', 'cloudüí≠üî≥', ',', 'and', 'dreamt', 'under', 'a', 'starry', 'blanketüî≥.', 'Zara‚Äôs', 'tale', 'reminds', 'us', 'that', 'life', 'weaves', 'magic', 'in', 'ordinary', 'moments.üòç']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "1.tokenizer=TreebankWordTokenizer(): Creates an instance of the TreebankWordTokenizer.\n",
        "2.print(tokenizer.tokenize(text)): Tokenizes the input text using the TreebankWordTokenizer and prints the resulting list of tokens.\n",
        "\n",
        "The output is a list of tokens, where each token represents a word or punctuation mark in the input text. The TreebankWordTokenizer follows the tokenization conventions used in the Penn Treebank corpus, which includes splitting contractions and punctuation marks into separate tokens."
      ],
      "metadata": {
        "id": "II1jh7xYOFnR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer= TweetTokenizer()\n",
        "print(tokenizer.tokenize(text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9u6LGDVCQcBK",
        "outputId": "ab58886a-2fe6-40bf-921a-fee715d94463"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['In', 'the', 'heart', 'of', 'a', 'sun-kissed', 'village', ',', 'curious', 'squirrel', 'Zara', 'embarked', 'on', 'an', 'adventure', '.', 'Guided', 'by', 'Luna', '‚Äô', 's', 'riddles', 'and', 'enchanted', 'by', 'a', 'hidden', 'waterfall', ',', 'she', 'climbed', 'the', 'mystical', 'mountain', '.', '‚ù§', 'Ô∏è\\u200düî•', 'At', 'the', 'summit', ',', 'she', 'marveled', 'at', 'the', 'vibrant', 'sky', ',', 'whispered', 'secrets', 'to', 'a', 'banana-shaped', 'cloud', 'üí≠', 'üî≥', ',', 'and', 'dreamt', 'under', 'a', 'starry', 'blanket', 'üî≥', '.', 'Zara', '‚Äô', 's', 'tale', 'reminds', 'us', 'that', 'life', 'weaves', 'magic', 'in', 'ordinary', 'moments', '.', 'üòç']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tweet tokenizer\n",
        "\n",
        "When we want to apply tokenization in text data like tweets, the tokenizers mentioned above can‚Äôt produce practical tokens. Through this issue, NLTK has a rule based tokenizer special for tweets. We can split emojis into different words if we need them for tasks like sentiment analysis"
      ],
      "metadata": {
        "id": "q2CttUqgQiJ_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer=MWETokenizer()\n",
        "print(tokenizer.tokenize(word_tokenize(text)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6cfBtyQCtUN7",
        "outputId": "937f28cf-56e3-4ed5-ea89-3853f55e29b6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['In', 'the', 'heart', 'of', 'a', 'sun-kissed', 'village', ',', 'curious', 'squirrel', 'Zara', 'embarked', 'on', 'an', 'adventure', '.', 'Guided', 'by', 'Luna', '‚Äô', 's', 'riddles', 'and', 'enchanted', 'by', 'a', 'hidden', 'waterfall', ',', 'she', 'climbed', 'the', 'mystical', 'mountain.‚ù§Ô∏è\\u200düî•', 'At', 'the', 'summit', ',', 'she', 'marveled', 'at', 'the', 'vibrant', 'sky', ',', 'whispered', 'secrets', 'to', 'a', 'banana-shaped', 'cloudüí≠üî≥', ',', 'and', 'dreamt', 'under', 'a', 'starry', 'blanketüî≥', '.', 'Zara', '‚Äô', 's', 'tale', 'reminds', 'us', 'that', 'life', 'weaves', 'magic', 'in', 'ordinary', 'moments.üòç']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer=MWETokenizer()\n",
        "tokenizer.add_mwe(('sun-kissed', 'village'))\n",
        "tokenizer.add_mwe(('banana-shaped', 'cloud'))\n",
        "print(tokenizer.tokenize(word_tokenize(text)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OoDpMnyot6nN",
        "outputId": "6ff48e20-c5c4-4000-d38d-9f2a54925572"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['In', 'the', 'heart', 'of', 'a', 'sun-kissed_village', ',', 'curious', 'squirrel', 'Zara', 'embarked', 'on', 'an', 'adventure', '.', 'Guided', 'by', 'Luna', '‚Äô', 's', 'riddles', 'and', 'enchanted', 'by', 'a', 'hidden', 'waterfall', ',', 'she', 'climbed', 'the', 'mystical', 'mountain.‚ù§Ô∏è\\u200düî•', 'At', 'the', 'summit', ',', 'she', 'marveled', 'at', 'the', 'vibrant', 'sky', ',', 'whispered', 'secrets', 'to', 'a', 'banana-shaped', 'cloudüí≠üî≥', ',', 'and', 'dreamt', 'under', 'a', 'starry', 'blanketüî≥', '.', 'Zara', '‚Äô', 's', 'tale', 'reminds', 'us', 'that', 'life', 'weaves', 'magic', 'in', 'ordinary', 'moments.üòç']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "NLTK‚Äôs multi-word expression tokenizer (MWETokenizer) provides a function add_mwe() that allows the user to enter multiple word expressions before using the tokenizer on the text. More simply, it can merge multi-word expressions into single tokens."
      ],
      "metadata": {
        "id": "F8Ooxya4QJhe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from textblob import TextBlob\n",
        "blob_object =TextBlob (text)\n",
        "# Word tokenization of the text\n",
        "text_words=blob_object.words\n",
        "# To see all tokens.\n",
        "print(text_words)\n",
        "# To count the number of tokens\n",
        "print(len(text_words))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ht0ZRQPouWFh",
        "outputId": "18a489ad-c15a-4c4f-9fe6-a676c3814b43"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['In', 'the', 'heart', 'of', 'a', 'sun-kissed', 'village', 'curious', 'squirrel', 'Zara', 'embarked', 'on', 'an', 'adventure', 'Guided', 'by', 'Luna', '‚Äô', 's', 'riddles', 'and', 'enchanted', 'by', 'a', 'hidden', 'waterfall', 'she', 'climbed', 'the', 'mystical', 'mountain.‚ù§Ô∏è\\u200düî•', 'At', 'the', 'summit', 'she', 'marveled', 'at', 'the', 'vibrant', 'sky', 'whispered', 'secrets', 'to', 'a', 'banana-shaped', 'cloudüí≠üî≥', 'and', 'dreamt', 'under', 'a', 'starry', 'blanketüî≥', 'Zara', '‚Äô', 's', 'tale', 'reminds', 'us', 'that', 'life', 'weaves', 'magic', 'in', 'ordinary', 'moments.üòç']\n",
            "65\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "TextBlob\n",
        "\n",
        " is a Python library for processing textual data. It provides a consistent API for diving into common natural language processing (NLP) tasks such as part-of-speech tagging, noun phrase extraction, sentiment analysis, classification, translation, and more."
      ],
      "metadata": {
        "id": "O2rEstTZP8Jo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.utils import tokenize\n",
        "list(tokenize(text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BJitAkblvIdZ",
        "outputId": "5a6e1b9f-bf31-4a20-85c3-3ea5eae17c11"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['In',\n",
              " 'the',\n",
              " 'heart',\n",
              " 'of',\n",
              " 'a',\n",
              " 'sun',\n",
              " 'kissed',\n",
              " 'village',\n",
              " 'curious',\n",
              " 'squirrel',\n",
              " 'Zara',\n",
              " 'embarked',\n",
              " 'on',\n",
              " 'an',\n",
              " 'adventure',\n",
              " 'Guided',\n",
              " 'by',\n",
              " 'Luna',\n",
              " 's',\n",
              " 'riddles',\n",
              " 'and',\n",
              " 'enchanted',\n",
              " 'by',\n",
              " 'a',\n",
              " 'hidden',\n",
              " 'waterfall',\n",
              " 'she',\n",
              " 'climbed',\n",
              " 'the',\n",
              " 'mystical',\n",
              " 'mountain',\n",
              " 'At',\n",
              " 'the',\n",
              " 'summit',\n",
              " 'she',\n",
              " 'marveled',\n",
              " 'at',\n",
              " 'the',\n",
              " 'vibrant',\n",
              " 'sky',\n",
              " 'whispered',\n",
              " 'secrets',\n",
              " 'to',\n",
              " 'a',\n",
              " 'banana',\n",
              " 'shaped',\n",
              " 'cloud',\n",
              " 'and',\n",
              " 'dreamt',\n",
              " 'under',\n",
              " 'a',\n",
              " 'starry',\n",
              " 'blanket',\n",
              " 'Zara',\n",
              " 's',\n",
              " 'tale',\n",
              " 'reminds',\n",
              " 'us',\n",
              " 'that',\n",
              " 'life',\n",
              " 'weaves',\n",
              " 'magic',\n",
              " 'in',\n",
              " 'ordinary',\n",
              " 'moments']"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Gensim\n",
        "\n",
        " is a Python library for topic modeling, document indexing, and similarity retrieval with large corpora. The target audience is the natural language processing (NLP) and information retrieval (IR) community. It offers utility functions for tokenization."
      ],
      "metadata": {
        "id": "UtDnXDtJP442"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.text import text_to_word_sequence\n",
        "ntoken= Tokenizer(num_words=20)\n",
        "ntoken.fit_on_texts(text)\n",
        "list_words= text_to_word_sequence(text)\n",
        "print(list_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wBH_tBfFv0h6",
        "outputId": "b8e1e6b0-1622-46a2-b409-3d35d8dff682"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['in', 'the', 'heart', 'of', 'a', 'sun', 'kissed', 'village', 'curious', 'squirrel', 'zara', 'embarked', 'on', 'an', 'adventure', 'guided', 'by', 'luna‚Äôs', 'riddles', 'and', 'enchanted', 'by', 'a', 'hidden', 'waterfall', 'she', 'climbed', 'the', 'mystical', 'mountain', '‚ù§Ô∏è\\u200düî•', 'at', 'the', 'summit', 'she', 'marveled', 'at', 'the', 'vibrant', 'sky', 'whispered', 'secrets', 'to', 'a', 'banana', 'shaped', 'cloudüí≠üî≥', 'and', 'dreamt', 'under', 'a', 'starry', 'blanketüî≥', 'zara‚Äôs', 'tale', 'reminds', 'us', 'that', 'life', 'weaves', 'magic', 'in', 'ordinary', 'moments', 'üòç']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "KERAS\n",
        "\n",
        "\n",
        "opensource library is one of the most reliable deep learning frameworks. To perform tokenization we use: text_to_word_sequence method from the Class Keras.preprocessing.text class. The great thing about Keras is converting the alphabet in a lower case before tokenizing it, which can be quite a time-saver."
      ],
      "metadata": {
        "id": "wsYAfLlhPmiy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Applications:\n",
        "\n",
        "This immediately turns an unstructured string (text document) into a numerical data structure suitable for machine learning. They can also be used directly by a computer to trigger useful actions and responses. Or they might be used in a machine learning pipeline as features that trigger more complex decisions or behavior."
      ],
      "metadata": {
        "id": "OMyq_SEITote"
      }
    }
  ]
}